{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Sentiment Analysis**\n",
    "With transformers from HuggingFace: https://huggingface.co/transformers/quicktour.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sentiment analysis, also called opinion mining, is the process of understanding the opinion of an author about a subject. In other words, \"What is the emotion or opinion of the author of the text about the subject discussed?\n",
    "\n",
    "In a sentiment analysis system, depending on the context, we usually have 3 elements: First is the **opinion or an emotion**. An opinion (also called \"polarity\") can be positive, neutral or negative. An emotion could be qualitative (like joy, surprise, or anger) or quantitative (like rating a movie on the scale from 1 to 10).\n",
    "\n",
    "The second element in a sentiment analysis system is the **subject that is being talked about**, such as a book, a movie, or a product. Sometimes one opinion could discuss multiple aspects of the same subject. For example: \"The camera on this phone is great but its battery life is rather disappointing.\"\" The third element is the **opinion holder**, or entity, expressing the opinion.\n",
    "\n",
    "Sentiment analysis has many practical applications. In social media monitoring, we want to know how they are talking about it. We can also find sentiment on forums, blogs, and the news. Most brands analyze all of these sources to enrich their understanding of how customers interact with their brand, what they are happy or unhappy about, and what matters most to consumers. Sentiment analysis is thus very important in brand monitoring, and in fields such as customer and product analytics and market research and analysis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Sentiment analysis types and approaches**\n",
    "\n",
    "Sentiment analysis tasks can be carried out at different levels of granularity. \n",
    "First is document level. This is when we look at the whole review of a product, for example. \n",
    "\n",
    "Second is the sentence level. This refers to determining whether the opinion expressed in each sentence is positive, negative, or neutral. \n",
    "\n",
    "The last level of granularity is the aspect level. The aspect refers to expressing opinions about different features of a product. \n",
    "\n",
    "Imagine a sentence such as \"The camera in this phone is pretty good but the battery life is disappointing.\" It expresses both positive and negative opinions about a phone and we might want to be able to say which features of the product clients like and which they don't.\n",
    "\n",
    "The algorithms used for sentiment analysis could be split into 2 main categories. \n",
    "\n",
    "- The first is **rule or lexicon based**. Such methods most commonly have a predefined list of words with a valence score. For example, nice could be +2, good +1, terrible -3, and so on. The algorithm then matches the words from the lexicon to the words in the text and either sums or averages the scores in some way. \n",
    "\n",
    "As an example, let's take the sentence, 'Today was a good day.' Each word gets a score, and to get the total valence we sum the words. In this case, we have a positive sentence. \n",
    "\n",
    "- A second category is automated systems, which are based on machine learning. \n",
    "\n",
    "The task is usually modeled as a classification problem where using some historical data with known sentiment, we need to predict the sentiment of a new piece of text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = \"Today was a good day\"\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "my_valence = TextBlob(text)\n",
    "my_valence.sentiment\n",
    "\n",
    "# Sentiment(polarity=0.7, subjectivity=0.60)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Valence of a sentence**\n",
    "\n",
    "We can calculate the valence score of a text, using Python's textblob library. We continue working with our 'Today was a good day' string. We import the TextBlob function from the textblob package and apply it to our string. TextBlob  has obtained some natural language processing skills. \n",
    "\n",
    "We are interested in its sentiment; that's why we call sentiment on our TextBlob. The sentiment property returns a tuple: polarity, which is measured on the scale from [-1.0 to 1.0], where -1.0 is very negative, 0 is neutral and +1.0 is very positive. \n",
    "\n",
    "Our example 'Today was a good day' carries positive emotion and thus will have a positive **polarity** score: 0.7. The second element in the tuple displays the **subjectivity**, measured from [0.0 to 1.0] where 0.0 is very objective and 1.0 is very subjective. So our example is rather positive and subjective.\n",
    "\n",
    "### **Automated or rule-based?**\n",
    "\n",
    "A machine learning sentiment analysis relies on having labeled historical data whereas lexicon-based methods rely on having manually created rules or dictionaries. \n",
    "\n",
    "Lexicon-based methods fail at certain tasks because the polarity of words might change with the problem, which will not be reflected in a predefined dictionary. \n",
    "\n",
    "However, lexicon-based approaches can be quite fast, whereas Machine learning models might take a while to train. At the same time, machine learning models can be quite powerful. So, the jury is still out on that one. Many people find that a hybrid approach tends to work best in many, usually complex scenarios."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word Cloud\n",
    "\n",
    "A word cloud is an image composed of words with different sizes and colors. They can be especially useful in sentiment analysis. \n",
    "\n",
    "Word clouds (also called tag clouds) are used across different contexts. In the most common type of word clouds the size of the text corresponds to the frequency of the word. The more frequent a word is, the bigger and bolder it will appear on the word cloud.\n",
    "\n",
    "Why are word clouds so popular? First of all, they can reveal the essential. We saw in our word cloud, the word Titanic really popped out. Second, unless told otherwise, they will plot all the words in a text, and a quick scan of the image can provide an overall sense of the text. Last but not least, they are easy to understand and quite fun. However, they have their drawbacks. Sometimes they tend to work less well. All the words plotted on the cloud might seem unrelated and it could be difficult to draw a conclusion based on a crowded word cloud. Secondly, if the text we work with is large, a word cloud might require quite a lot of preprocessing steps before it appears sensible and uncluttered.\n",
    "\n",
    "We can use the WordCloud function from the wordcloud package. We will have to import matplotlib.pyplot as well, which will allow wordcloud to plot on its base."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "two_cities = \"It was the best of times, it was the worst of times,         it was the age of wisdom, it was the age of foolishness,         it was the epoch of belief, it was the epoch of incredulity,         it was the season of Light, it was the season of Darkness,         it was the spring of hope, it was the winter of despair,         we had everything before us, we had nothing before us,         we were all going direct to Heaven, we were all going        direct the other way – in short, the period was so far        like the present period, that some of its noisiest        authorities insisted on its being received, for good        or for evil, in the superlative degree of comparison only.\""
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b5d77f34d68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtwo_cities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"It was the best of times, it was the worst of times,         it was the age of wisdom, it was the age of foolishness,         it was the epoch of belief, it was the epoch of incredulity,         it was the season of Light, it was the season of Darkness,         it was the spring of hope, it was the winter of despair,         we had everything before us, we had nothing before us,         we were all going direct to Heaven, we were all going        direct the other way – in short, the period was so far        like the present period, that some of its noisiest        authorities insisted on its being received, for good        or for evil, in the superlative degree of comparison only.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cloud_two_cities = WordCloud().generate(two_cities)\n",
    "plt.imshow(cloud_two_cities, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the word cloud function \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(background_color='white', stopwords=my_stopwords).generate(descriptions)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "1f1ade11a66f379951eb785ed5ab9940defd282e53b6a037182efca5c962c31b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}