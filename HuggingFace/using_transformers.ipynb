{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Using transformers**\n",
    "\n",
    "We used Transformer models for different tasks using the high-level pipeline API. Although this API is powerful and convenient, it‚Äôs important to understand how it works under the hood so we have the flexibility to solve other problems.\n",
    "\n",
    "In this chapter, you will learn:\n",
    "\n",
    "- How to use tokenizers and models to replicate the pipeline API‚Äôs behavior\n",
    "- How to load and save models and tokenizers\n",
    "- Different tokenization approaches, such as word-based, character-based, and subword-based\n",
    "- How to handle multiple sentences of varying lengths\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "As you saw in Chapter 1, Transformer models are usually very large. With millions to tens of billions of parameters, training and deploying these models is a complicated undertaking. Furthermore, with new models being released on a near-daily basis and each having its own implementation, trying them all out is no easy task.\n",
    "\n",
    "The ü§ó Transformers library was created to solve this problem. **Its goal is to provide a single API through which any Transformer model can be loaded, trained, and saved**. The library‚Äôs main features are:\n",
    "\n",
    "- **Ease of use**: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.\n",
    "- **Flexibility**: At their core, all models are simple PyTorch nn.Module or TensorFlow tf.keras.Model classes and can be handled like any other models in their respective machine learning (ML) frameworks.\n",
    "- **Simplicity**: Hardly any abstractions are made across the library. The ‚ÄúAll in one file‚Äù is a core concept: a model‚Äôs forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.\n",
    "\n",
    "This last feature makes ü§ó Transformers quite different from other ML libraries. The models are not built on modules that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.\n",
    "\n",
    "This chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the pipeline API introduced in Chapter 1. Next, we‚Äôll discuss the model API: we‚Äôll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions.\n",
    "\n",
    "Then we‚Äôll look at the tokenizer API, which is the other main component of the pipeline. **Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed**. Finally, we‚Äôll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level tokenizer function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pipeline consists of three stages: \n",
    "- Tokenizer\n",
    "- Model \n",
    "- Postprocessing\n",
    "\n",
    "# **Preprocessing with a tokenizer**\n",
    "\n",
    "Transformer models can‚Äôt process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a tokenizer, which will be responsible for:\n",
    "\n",
    "- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "- Mapping each token to an integer\n",
    "Adding additional inputs that may be useful to the model\n",
    "\n",
    "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the Model Hub. To do this, we use the AutoTokenizer class and its from_pretrained method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model‚Äôs tokenizer and cache it (so it‚Äôs only downloaded the first time you run the code below).\n",
    "\n",
    "Since the default checkpoint of the sentiment-analysis pipeline is distilbert-base-uncased-finetuned-sst-2-english (you can see its model card here), we run the following:\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "Once we have the tokenizer, we can directly pass our sentences to it and we‚Äôll get back a dictionary that‚Äôs ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.\n",
    "\n",
    "You can use ü§ó Transformers without having to worry about which ML framework is used as a backend; it might be PyTorch or TensorFlow, or Flax for some models. However, Transformer models only accept tensors as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It‚Äôs effectively a tensor; other ML frameworks‚Äô tensors behave similarly, and are usually as simple to instantiate as NumPy arrays.\n",
    "\n",
    "To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument:\n",
    "\n",
    "\n",
    "    raw_inputs = [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    "    inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    print(inputs)\n",
    "\n",
    "Don‚Äôt worry about padding and truncation just yet; we‚Äôll explain those later. The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result).\n",
    "\n",
    "Here‚Äôs what the results look like as TensorFlow tensors:\n",
    "\n",
    "    {\n",
    "        'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
    "            array([\n",
    "                [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
    "                [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]\n",
    "            ], dtype=int32)>, \n",
    "        'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
    "            array([\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            ], dtype=int32)>\n",
    "    }\n",
    "\n",
    "The output itself is a dictionary containing two keys, input_ids and attention_mask. input_ids contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. We‚Äôll explain what the attention_mask is later in this chapter.\n",
    "\n",
    "## **Going through the model**\n",
    "We can download our pretrained model the same way we did with our tokenizer. ü§ó Transformers provides an AutoModel class which also has a from_pretrained method:\n",
    "\n",
    "    from transformers import TFAutoModel\n",
    "\n",
    "    checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    model = TFAutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "In this code snippet, we have downloaded the same checkpoint we used in our pipeline before (it should actually have been cached already) and instantiated a model with it.\n",
    "\n",
    "This architecture contains only the base Transformer module: given some inputs, it outputs what we‚Äôll call hidden states, also known as features. For each model input, we‚Äôll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model.\n",
    "\n",
    "If this doesn‚Äôt make sense, don‚Äôt worry about it. We‚Äôll explain it all later.\n",
    "\n",
    "While these hidden states can be useful on their own, they‚Äôre usually inputs to another part of the model, known as the head. In Chapter 1, the different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it.\n",
    "\n",
    "**A high-dimensional vector?**\n",
    "\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "- **Batch size**: The number of sequences processed at a time (2 in our example).\n",
    "- **Sequence length**: The length of the numerical representation of the sequence (16 in our example).\n",
    "- **Hidden size**: The vector dimension of each model input.\n",
    "\n",
    "It is said to be ‚Äúhigh dimensional‚Äù because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more).\n",
    "\n",
    "We can see this if we feed the inputs we preprocessed to our model:\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs.last_hidden_state.shape)\n",
    "\n",
    "    (2, 16, 768)\n",
    "\n",
    "Note that the outputs of ü§ó Transformers models behave like namedtuples or dictionaries. You can access the elements by attributes (like we did) or by key (outputs[\"last_hidden_state\"]), or even by index if you know exactly where the thing you are looking for is (outputs[0]).\n",
    "\n",
    "**Model heads: Making sense out of numbers**\n",
    "\n",
    "The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:\n",
    "\n",
    "A Transformer network alongside its head.\n",
    "\n",
    "The output of the Transformer model is sent directly to the model head to be processed.\n",
    "\n",
    "In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences.\n",
    "\n",
    "There are many different architectures available in ü§ó Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:\n",
    "\n",
    "- *Model (retrieve the hidden states)\n",
    "- *ForCausalLM\n",
    "- *ForMaskedLM\n",
    "- *ForMultipleChoice\n",
    "- *ForQuestionAnswering\n",
    "- *ForSequenceClassification\n",
    "- *ForTokenClassification\n",
    "- and others ü§ó\n",
    "\n",
    "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won‚Äôt actually use the AutoModel class, but AutoModelForSequenceClassification:\n",
    "\n",
    "    from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "    checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "    outputs = model(inputs)\n",
    "\n",
    "Now if we look at the shape of our inputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):\n",
    "\n",
    "    print(outputs.logits.shape)\n",
    "\n",
    "    (2, 2)\n",
    "\n",
    "Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2.\n",
    "\n",
    "## **Postprocessing the output**\n",
    "The values we get as output from our model don‚Äôt necessarily make sense by themselves. Let‚Äôs take a look:\n",
    "\n",
    "    print(outputs.logits)\n",
    "\n",
    "    <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "    array([[-1.5606991,  1.6122842],\n",
    "           [ 4.169231 , -3.3464472]], dtype=float32)>\n",
    "\n",
    "Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. **To be converted to probabilities, they need to go through a SoftMax layer** (all ü§ó Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "    print(predictions)\n",
    "    \n",
    "    tf.Tensor(\n",
    "    [[4.01951671e-02 9.59804833e-01]\n",
    "    [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)\n",
    "\n",
    "Now we can see that the model predicted [0.0402, 0.9598] for the first sentence and [0.9995, 0.0005] for the second one. These are recognizable probability scores.\n",
    "\n",
    "To get the labels corresponding to each position, we can inspect the id2label attribute of the model config (more on this in the next section):\n",
    "\n",
    "    model.config.id2label\n",
    "\n",
    "    {0: 'NEGATIVE', 1: 'POSITIVE'}\n",
    "\n",
    "Now we can conclude that the model predicted the following:\n",
    "\n",
    "- First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
    "- Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005\n",
    "\n",
    "We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing! Now let‚Äôs take some time to dive deeper into each of those steps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Models** \n",
    "\n",
    "## **Creating a Transformer**\n",
    "The first thing we‚Äôll need to do to initialize a BERT model is load a configuration object:\n",
    "\n",
    "  from transformers import BertConfig, TFBertModel\n",
    "\n",
    "  # Building the config\n",
    "  config = BertConfig()\n",
    "\n",
    "  # Building the model from the config\n",
    "  model = TFBertModel(config)\n",
    "\n",
    "The configuration contains many attributes that are used to build the model:\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    BertConfig {\n",
    "      [...]\n",
    "      \"hidden_size\": 768,\n",
    "      \"intermediate_size\": 3072,\n",
    "      \"max_position_embeddings\": 512,\n",
    "      \"num_attention_heads\": 12,\n",
    "      \"num_hidden_layers\": 12,\n",
    "      [...]\n",
    "    }\n",
    "\n",
    "While you haven‚Äôt seen what all of these attributes do yet, you should recognize some of them: the hidden_size attribute defines the size of the hidden_states vector, and num_hidden_layers defines the number of layers the Transformer model has.\n",
    "\n",
    "**Different loading methods**\n",
    "\n",
    "Creating a model from the default configuration initializes it with random values:\n",
    "\n",
    "    from transformers import BertConfig, TFBertModel\n",
    "\n",
    "    config = BertConfig()\n",
    "    model = TFBertModel(config)\n",
    "\n",
    "# Model is randomly initialized!\n",
    "The model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand, but as you saw in Chapter 1, this would require a long time and a lot of data, and it would have a non-negligible environmental impact. To avoid unnecessary and duplicated effort, it‚Äôs imperative to be able to share and reuse models that have already been trained.\n",
    "\n",
    "Loading a Transformer model that is already trained is simple ‚Äî we can do this using the from_pretrained method:\n",
    "\n",
    "    from transformers import TFBertModel\n",
    "\n",
    "    model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "As you saw earlier, we could replace TFBertModel with the equivalent TFAutoModel class. We‚Äôll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task).\n",
    "\n",
    "In the code sample above we didn‚Äôt use BertConfig, and instead loaded a pretrained model via the bert-base-cased identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its model card.\n",
    "\n",
    "This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.\n",
    "\n",
    "The weights have been downloaded and cached (so future calls to the from_pretrained method won‚Äôt re-download them) in the cache folder, which defaults to ~/.cache/huggingface/transformers. You can customize your cache folder by setting the HF_HOME environment variable.\n",
    "\n",
    "The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture. The entire list of available BERT checkpoints can be found here.\n",
    "\n",
    "## **Saving methods**\n",
    "Saving a model is as easy as loading one ‚Äî we use the save_pretrained method, which is analogous to the from_pretrained method:\n",
    "\n",
    "    model.save_pretrained(\"directory_on_my_computer\")\n",
    "\n",
    "This saves two files to your disk:\n",
    "\n",
    "    ls directory_on_my_computer\n",
    "\n",
    "    config.json tf_model.h5\n",
    "\n",
    "If you take a look at the config.json file, you‚Äôll recognize the attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what ü§ó Transformers version you were using when you last saved the checkpoint.\n",
    "\n",
    "The tf_model.h5 file is known as the state dictionary; it contains all your model‚Äôs weights. The two files go hand in hand; the configuration is necessary to know your model‚Äôs architecture, while the model weights are your model‚Äôs parameters.\n",
    "\n",
    "## **Using a Transformer model for inference**\n",
    "Now that you know how to load and save a model, let‚Äôs try using it to make some predictions. Transformer models can only process numbers ‚Äî numbers that the tokenizer generates. But before we discuss tokenizers, let‚Äôs explore what inputs the model accepts.\n",
    "\n",
    "Tokenizers can take care of casting the inputs to the appropriate framework‚Äôs tensors, but to help you understand what‚Äôs going on, we‚Äôll take a quick look at what must be done before sending the inputs to the model.\n",
    "\n",
    "Let‚Äôs say we have a couple of sequences:\n",
    "\n",
    "    sequences = [\n",
    "      \"Hello!\",\n",
    "      \"Cool.\",\n",
    "      \"Nice!\"\n",
    "    ]\n",
    "The tokenizer converts these to vocabulary indices which are typically called input IDs. Each sequence is now a list of numbers! The resulting output is:\n",
    "\n",
    "    encoded_sequences = [\n",
    "      [ 101, 7592,  999,  102],\n",
    "      [ 101, 4658, 1012,  102],\n",
    "      [ 101, 3835,  999,  102]\n",
    "    ]\n",
    "\n",
    "This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This ‚Äúarray‚Äù is already of rectangular shape, so converting it to a tensor is easy:\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    model_inputs = tf.constant(encoded_sequences)\n",
    "\n",
    "Using the tensors as inputs to the model\n",
    "Making use of the tensors with the model is extremely simple ‚Äî we just call the model with the inputs:\n",
    "\n",
    "    output = model(model_inputs)\n",
    "  \n",
    "While the model accepts a lot of different arguments, only the input IDs are necessary. We‚Äôll explain what the other arguments do and when they are required later, but first we need to take a closer look at the tokenizers that build the inputs that a Transformer model can understand."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Tokenizers** \n",
    "\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "1f1ade11a66f379951eb785ed5ab9940defd282e53b6a037182efca5c962c31b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}