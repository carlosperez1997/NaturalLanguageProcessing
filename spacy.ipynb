{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "https://realpython.com/natural-language-processing-spacy-python/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **SpaCy**\n",
    "\n",
    "SpaCy is a NLP library similar to Gensim, but with different implementations, including a particular focus on creating NLP pipelines to generate models and corpora. SpaCy is open-source and has several extra libraries and tools built by the same team, including Displacy - a visualization tool for viewing parse trees which uses Node-js to create interactive text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.entity\n",
    "doc = nlp(\"\"\"Berlin is the capital of Germany;\n",
    "                 and the residence of Chancellor Angela Merkel.\"\"\")\n",
    "\n",
    "print(doc.ents)\n",
    "print(doc.ents[0], doc.ents[0].label_)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(Berlin, Germany, Angela Merkel)\n",
      "Berlin GPE\n",
      "Berlin be the capital of Germany ; \n",
      "                  and the residence of Chancellor Angela Merkel .\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Lemmatization** \n",
    "\n",
    "Convert word to its base form: reducing, reduces, reduced, reduction => reduce // n’t => not \n",
    "\n",
    "When you pass the string into nlp, spaCy automatically performs lemmatization by default. Therefore, generating lemmas is identical to generating tokens except that we extract token.lemma_ in each iteration inside the list comprehension instead of token.text.\n",
    "\n",
    "Spacy comes with informal language corpora, allowing you to more easily find entities in documents like Tweets and chat messages. It's a quickly growing library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "article = \"\"\"If we hope to one day leave Earth and explore the universe, our bodies are going to have to get a lot better at surviving the harsh conditions of space. \n",
    "Using synthetic biology since September, Lisa Nip hopes to harness special powers from microbes on Earth -- such as the ability to withstand radiation -- to make humans more fit for exploring space. \n",
    "\"Lisa is approaching a time during which we'll have the capacity to decide our own genetic destiny,\" Nip says. \n",
    "\"Augmenting the human body with new abilities is no longer a question of how, but of when. Carlos P. \"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import spacy\n",
    "\n",
    "#nlp = spacy.load('en',tagger=False, parser=False, matcher=False)\n",
    "nlp = spacy.load('en_core_web_sm',tagger=False, parser=False, matcher=False)\n",
    "doc = nlp(article)\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DATE one day\n",
      "LOC Earth\n",
      "DATE September\n",
      "PERSON Lisa Nip\n",
      "LOC Earth\n",
      "PERSON Lisa\n",
      "PERSON Carlos P.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Multilingual NER with polyglot**\n",
    "\n",
    "Polyglot is yet another natural language processing library which uses word vectors to perform simple tasks such as entity recognition.\n",
    "\n",
    "The main benefit and difference of using Polyglot, however, is the wide variety of languages it supports. Polyglot has word embeddings for more than 130 languages! For this reason, you can even use it for tasks like transliteration, as shown here translating some english text into arabic. Transliteration is the ability to translate text by swapping characters from one language to another."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from polyglot.text import Text\n",
    "\n",
    "text = \"\"\"El presidente de la Generalitat de Cataluña,\n",
    "                 Carles Puigdemont, ha afirmado hoy a la alcaldesa\n",
    "                 de Madrid, Manuela Carmena, que en su etapa de\n",
    "                 alcalde de Girona (de julio de 2011 a enero de 2016)\n",
    "                 hizo una gran promoción de Madrid.\"\"\"\n",
    "ptext = Text(text)\n",
    "ptext.entities\n",
    "\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in ptext.entities]\n",
    "\n",
    "print(entities)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'icu'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7deab75e8842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m text = \"\"\"El presidente de la Generalitat de Cataluña,\n\u001b[1;32m      4\u001b[0m                  \u001b[0mCarles\u001b[0m \u001b[0mPuigdemont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mha\u001b[0m \u001b[0mafirmado\u001b[0m \u001b[0mhoy\u001b[0m \u001b[0ma\u001b[0m \u001b[0mla\u001b[0m \u001b[0malcaldesa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mde\u001b[0m \u001b[0mMadrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mManuela\u001b[0m \u001b[0mCarmena\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mque\u001b[0m \u001b[0men\u001b[0m \u001b[0msu\u001b[0m \u001b[0metapa\u001b[0m \u001b[0mde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/polyglot/text.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/polyglot/detect/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Detector'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Language'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/polyglot/detect/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0micu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpycld2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcld2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'icu'"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "1f1ade11a66f379951eb785ed5ab9940defd282e53b6a037182efca5c962c31b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}