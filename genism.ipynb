{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Genism**\n",
    "\n",
    "**Gensim** is a popular open-source natural language processing library. It uses top academic models to perform complex tasks like building document or word vectors, corpora and performing topic identification and document comparisons.\n",
    "\n",
    "A word embedding or vector is trained from a larger corpus and is a multi-dimensional representation of a word or document. You can think of it as a multi-dimensional array normally with sparse features (lots of zeros and some ones). With these vectors, we can then see relationships among the words or documents based on how near or far they are and also what similar comparisons we find. For example, in this graphic we can see that the vector operation king minus queen is approximately equal to man minus woman. Or that Spain is to Madrid as Italy is to Rome. The deep learning algorithm used to create word vectors has been able to distill this meaning based on how those words are used throughout the text.\n",
    "\n",
    "A corpus (or if plural, corpora) is a set of texts used to help perform natural language processing tasks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "my_documents = ['The movie was about a spaceship and aliens.','I really liked the movie!','Awesome action scenes, but boring characters.','The movie was awful! I hate alien films.','Space is cool! I liked the movie.','More space films, please!',]\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary.token2id\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-58605b609185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'The movie was about a spaceship and aliens.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'I really liked the movie!'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Awesome action scenes, but boring characters.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'The movie was awful! I hate alien films.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Space is cool! I liked the movie.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'More space films, please!'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenized_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_documents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the Gensim corpus is a list of lists, each list item representing one document. Each document a series of tuples, the first item representing the tokenid from the dictionary and the second item representing the token frequency in the document.\n",
    "\n",
    "Gensim models can be easily saved, updated, and reused. Our dictionary can also be updated. This more advanced and feature rich bag-of-words can be used in future exercises."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.corpora.dictionary import Dictionary \n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += int(word_count)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "1f1ade11a66f379951eb785ed5ab9940defd282e53b6a037182efca5c962c31b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}